2018- 9-09 20:04:41 crawl_class.py[line:144] ERROR Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x000002351630FE80>, 'Connection to 175.22.99.112 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='175.22.99.112', port=8060): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002351630FE80>, 'Connection to 175.22.99.112 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 126, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='175.22.99.112', port=8060): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002351630FE80>, 'Connection to 175.22.99.112 timed out. (connect timeout=2)'))

2018- 9-09 20:04:41 crawl_class.py[line:145] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:04:43 crawl_class.py[line:144] ERROR Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x000002351636FF28>, 'Connection to 119.28.112.130 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='119.28.112.130', port=3128): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002351636FF28>, 'Connection to 119.28.112.130 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 126, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='119.28.112.130', port=3128): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002351636FF28>, 'Connection to 119.28.112.130 timed out. (connect timeout=2)'))

2018- 9-09 20:04:43 crawl_class.py[line:145] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:04:45 crawl_class.py[line:144] ERROR Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 128, in crawl
    data = parse_func(response)
TypeError: parse() missing 1 required positional argument: 'crawl_url'

2018- 9-09 20:04:45 crawl_class.py[line:145] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:05:21 crawl_class.py[line:145] ERROR Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x00000250CDD3FAC8>, 'Connection to 211.136.127.125 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='211.136.127.125', port=80): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000250CDD3FAC8>, 'Connection to 211.136.127.125 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 127, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='211.136.127.125', port=80): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000250CDD3FAC8>, 'Connection to 211.136.127.125 timed out. (connect timeout=2)'))

2018- 9-09 20:05:21 crawl_class.py[line:146] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:05:22 crawl_class.py[line:145] ERROR Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 129, in crawl
    data = parse_func(response)
TypeError: parse() missing 1 required positional argument: 'crawl_url'

2018- 9-09 20:05:22 crawl_class.py[line:146] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:05:22 crawl_class.py[line:145] ERROR Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 595, in urlopen
    self._prepare_proxy(conn)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 816, in _prepare_proxy
    conn.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 294, in connect
    self._tunnel()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 809, in _tunnel
    message.strip()))
OSError: Tunnel connection failed: 400 Bad Request

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.baidu.com', port=443): Max retries exceeded with url: /?dsp=ipad (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 400 Bad Request',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 127, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 640, in send
    history = [resp for resp in gen] if allow_redirects else []
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 640, in <listcomp>
    history = [resp for resp in gen] if allow_redirects else []
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 218, in resolve_redirects
    **adapter_kwargs
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 502, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPSConnectionPool(host='www.baidu.com', port=443): Max retries exceeded with url: /?dsp=ipad (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 400 Bad Request',)))

2018- 9-09 20:05:22 crawl_class.py[line:146] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:05:22 crawl_class.py[line:145] ERROR Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 387, in _make_request
    six.raise_from(e, None)
  File "<string>", line 2, in raise_from
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 383, in _make_request
    httplib_response = conn.getresponse()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1174, in getresponse
    response.begin()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 282, in begin
    version, status, reason = self._read_status()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 243, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\socket.py", line 571, in readinto
    return self._sock.recv_into(b)
ConnectionAbortedError: [WinError 10053] 你的主机中的软件中止了一个已建立的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='114.101.135.27', port=61234): Max retries exceeded with url: http://www.baidu.com/ (Caused by ProxyError('Cannot connect to proxy.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 127, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 502, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPConnectionPool(host='114.101.135.27', port=61234): Max retries exceeded with url: http://www.baidu.com/ (Caused by ProxyError('Cannot connect to proxy.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)))

2018- 9-09 20:05:22 crawl_class.py[line:146] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:05:22 crawl_class.py[line:145] ERROR Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 387, in _make_request
    six.raise_from(e, None)
  File "<string>", line 2, in raise_from
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 383, in _make_request
    httplib_response = conn.getresponse()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1174, in getresponse
    response.begin()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 282, in begin
    version, status, reason = self._read_status()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 251, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='61.136.187.13', port=8908): Max retries exceeded with url: http://www.baidu.com/ (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 127, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 502, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPConnectionPool(host='61.136.187.13', port=8908): Max retries exceeded with url: http://www.baidu.com/ (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

2018- 9-09 20:05:22 crawl_class.py[line:146] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:05:24 crawl_class.py[line:145] ERROR Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x00000250CAAB60B8>, 'Connection to 123.149.207.222 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='123.149.207.222', port=8888): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000250CAAB60B8>, 'Connection to 123.149.207.222 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 127, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='123.149.207.222', port=8888): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000250CAAB60B8>, 'Connection to 123.149.207.222 timed out. (connect timeout=2)'))

2018- 9-09 20:05:24 crawl_class.py[line:146] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:06:12 crawl_class.py[line:149] ERROR Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 133, in crawl
    data = parse_func(response)
TypeError: parse() missing 1 required positional argument: 'crawl_url'

2018- 9-09 20:06:12 crawl_class.py[line:150] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:06:12 crawl_class.py[line:149] ERROR Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 387, in _make_request
    six.raise_from(e, None)
  File "<string>", line 2, in raise_from
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 383, in _make_request
    httplib_response = conn.getresponse()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1174, in getresponse
    response.begin()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 282, in begin
    version, status, reason = self._read_status()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 251, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='117.71.159.66', port=61234): Max retries exceeded with url: http://www.baidu.com/ (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 131, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 502, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPConnectionPool(host='117.71.159.66', port=61234): Max retries exceeded with url: http://www.baidu.com/ (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

2018- 9-09 20:06:12 crawl_class.py[line:150] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:06:16 crawl_class.py[line:149] ERROR Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 595, in urlopen
    self._prepare_proxy(conn)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 816, in _prepare_proxy
    conn.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 284, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.VerifiedHTTPSConnection object at 0x0000024FBE0109E8>, 'Connection to 117.146.19.161 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.baidu.com', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x0000024FBE0109E8>, 'Connection to 117.146.19.161 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 131, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 640, in send
    history = [resp for resp in gen] if allow_redirects else []
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 640, in <listcomp>
    history = [resp for resp in gen] if allow_redirects else []
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 218, in resolve_redirects
    **adapter_kwargs
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='www.baidu.com', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x0000024FBE0109E8>, 'Connection to 117.146.19.161 timed out. (connect timeout=2)'))

2018- 9-09 20:06:16 crawl_class.py[line:150] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:06:18 crawl_class.py[line:149] ERROR Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x0000024FBE010F28>, 'Connection to 117.135.78.162 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='117.135.78.162', port=8060): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x0000024FBE010F28>, 'Connection to 117.135.78.162 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 131, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='117.135.78.162', port=8060): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x0000024FBE010F28>, 'Connection to 117.135.78.162 timed out. (connect timeout=2)'))

2018- 9-09 20:06:18 crawl_class.py[line:150] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:06:19 crawl_class.py[line:149] ERROR Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 150, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000024FBE010780>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='183.159.89.201', port=18118): Max retries exceeded with url: http://www.baidu.com/ (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024FBE010780>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 131, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 502, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPConnectionPool(host='183.159.89.201', port=18118): Max retries exceeded with url: http://www.baidu.com/ (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024FBE010780>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',)))

2018- 9-09 20:06:19 crawl_class.py[line:150] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:06:20 crawl_class.py[line:149] ERROR Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 387, in _make_request
    six.raise_from(e, None)
  File "<string>", line 2, in raise_from
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 383, in _make_request
    httplib_response = conn.getresponse()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1174, in getresponse
    response.begin()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 282, in begin
    version, status, reason = self._read_status()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 243, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\socket.py", line 571, in readinto
    return self._sock.recv_into(b)
ConnectionAbortedError: [WinError 10053] 你的主机中的软件中止了一个已建立的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='124.113.212.118', port=61234): Max retries exceeded with url: http://www.baidu.com/ (Caused by ProxyError('Cannot connect to proxy.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 131, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 502, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPConnectionPool(host='124.113.212.118', port=61234): Max retries exceeded with url: http://www.baidu.com/ (Caused by ProxyError('Cannot connect to proxy.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)))

2018- 9-09 20:06:20 crawl_class.py[line:150] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:06:22 crawl_class.py[line:149] ERROR Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 595, in urlopen
    self._prepare_proxy(conn)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 816, in _prepare_proxy
    conn.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 284, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.VerifiedHTTPSConnection object at 0x0000024FBE010710>, 'Connection to 59.63.43.173 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.baidu.com', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x0000024FBE010710>, 'Connection to 59.63.43.173 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 131, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 640, in send
    history = [resp for resp in gen] if allow_redirects else []
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 640, in <listcomp>
    history = [resp for resp in gen] if allow_redirects else []
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 218, in resolve_redirects
    **adapter_kwargs
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='www.baidu.com', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x0000024FBE010710>, 'Connection to 59.63.43.173 timed out. (connect timeout=2)'))

2018- 9-09 20:06:22 crawl_class.py[line:150] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:06:23 crawl_class.py[line:149] ERROR Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 150, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000024FBE01DEB8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='183.159.95.162', port=18118): Max retries exceeded with url: http://www.baidu.com/ (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024FBE01DEB8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 131, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 502, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPConnectionPool(host='183.159.95.162', port=18118): Max retries exceeded with url: http://www.baidu.com/ (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024FBE01DEB8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',)))

2018- 9-09 20:06:23 crawl_class.py[line:150] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:06:23 crawl_class.py[line:149] ERROR Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 387, in _make_request
    six.raise_from(e, None)
  File "<string>", line 2, in raise_from
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 383, in _make_request
    httplib_response = conn.getresponse()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1174, in getresponse
    response.begin()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 282, in begin
    version, status, reason = self._read_status()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 251, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='61.178.117.38', port=8908): Max retries exceeded with url: http://www.baidu.com/ (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 131, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 502, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPConnectionPool(host='61.178.117.38', port=8908): Max retries exceeded with url: http://www.baidu.com/ (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

2018- 9-09 20:06:23 crawl_class.py[line:150] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:06:29 crawl_class.py[line:149] ERROR Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\response.py", line 302, in _error_catcher
    yield
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\response.py", line 598, in read_chunked
    self._update_chunk_length()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\response.py", line 540, in _update_chunk_length
    line = self._fp.fp.readline()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\socket.py", line 571, in readinto
    return self._sock.recv_into(b)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\models.py", line 745, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\response.py", line 432, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\response.py", line 626, in read_chunked
    self._original_response.close()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\response.py", line 307, in _error_catcher
    raise ReadTimeoutError(self._pool, None, 'Read timed out.')
urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='39.106.4.27', port=9999): Read timed out.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 131, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 658, in send
    r.content
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\models.py", line 823, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\models.py", line 752, in generate
    raise ConnectionError(e)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='39.106.4.27', port=9999): Read timed out.

2018- 9-09 20:06:29 crawl_class.py[line:150] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:06:29 crawl_class.py[line:149] ERROR Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\response.py", line 266, in _decode
    data = self._decoder.decompress(data)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\response.py", line 66, in decompress
    return self._obj.decompress(data)
zlib.error: Error -3 while decompressing data: invalid distance too far back

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\models.py", line 745, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\response.py", line 436, in stream
    data = self.read(amt=amt, decode_content=decode_content)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\response.py", line 408, in read
    data = self._decode(data, decode_content, flush_decoder)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\response.py", line 271, in _decode
    "failed to decode it." % content_encoding, e)
urllib3.exceptions.DecodeError: ('Received response with content-encoding: gzip, but failed to decode it.', error('Error -3 while decompressing data: invalid distance too far back',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 131, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 658, in send
    r.content
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\models.py", line 823, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\models.py", line 750, in generate
    raise ContentDecodingError(e)
requests.exceptions.ContentDecodingError: ('Received response with content-encoding: gzip, but failed to decode it.', error('Error -3 while decompressing data: invalid distance too far back',))

2018- 9-09 20:06:29 crawl_class.py[line:150] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:06:29 crawl_class.py[line:149] ERROR Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 387, in _make_request
    six.raise_from(e, None)
  File "<string>", line 2, in raise_from
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 383, in _make_request
    httplib_response = conn.getresponse()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1174, in getresponse
    response.begin()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 282, in begin
    version, status, reason = self._read_status()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 243, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\socket.py", line 571, in readinto
    return self._sock.recv_into(b)
ConnectionAbortedError: [WinError 10053] 你的主机中的软件中止了一个已建立的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='27.29.157.1', port=61234): Max retries exceeded with url: http://www.baidu.com/ (Caused by ProxyError('Cannot connect to proxy.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 131, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 502, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPConnectionPool(host='27.29.157.1', port=61234): Max retries exceeded with url: http://www.baidu.com/ (Caused by ProxyError('Cannot connect to proxy.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)))

2018- 9-09 20:06:29 crawl_class.py[line:150] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:06:29 crawl_class.py[line:149] ERROR Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 387, in _make_request
    six.raise_from(e, None)
  File "<string>", line 2, in raise_from
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 383, in _make_request
    httplib_response = conn.getresponse()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1174, in getresponse
    response.begin()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 282, in begin
    version, status, reason = self._read_status()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 251, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='182.34.16.172', port=61234): Max retries exceeded with url: http://www.baidu.com/ (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 131, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 502, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPConnectionPool(host='182.34.16.172', port=61234): Max retries exceeded with url: http://www.baidu.com/ (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

2018- 9-09 20:06:29 crawl_class.py[line:150] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:07:27 crawl_class.py[line:149] ERROR Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x000001CF51A32588>, 'Connection to 223.241.78.143 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='223.241.78.143', port=18118): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000001CF51A32588>, 'Connection to 223.241.78.143 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 131, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='223.241.78.143', port=18118): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000001CF51A32588>, 'Connection to 223.241.78.143 timed out. (connect timeout=2)'))

2018- 9-09 20:07:28 crawl_class.py[line:150] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:07:35 crawl_class.py[line:149] ERROR Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x000001CF51AA0438>, 'Connection to 110.73.51.55 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='110.73.51.55', port=8123): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000001CF51AA0438>, 'Connection to 110.73.51.55 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 131, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='110.73.51.55', port=8123): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000001CF51AA0438>, 'Connection to 110.73.51.55 timed out. (connect timeout=2)'))

2018- 9-09 20:07:35 crawl_class.py[line:150] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:09:04 crawl_class.py[line:149] ERROR Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 595, in urlopen
    self._prepare_proxy(conn)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 816, in _prepare_proxy
    conn.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 294, in connect
    self._tunnel()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 804, in _tunnel
    (version, code, message) = response._read_status()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 264, in _read_status
    raise BadStatusLine(line)
http.client.BadStatusLine:  HTTP/1.0 500 Internal Server Error


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 357, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\packages\six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 595, in urlopen
    self._prepare_proxy(conn)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 816, in _prepare_proxy
    conn.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 294, in connect
    self._tunnel()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 804, in _tunnel
    (version, code, message) = response._read_status()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 264, in _read_status
    raise BadStatusLine(line)
urllib3.exceptions.ProtocolError: ('Connection aborted.', BadStatusLine('\x15\x03\x01\x00\x02\x02\x16HTTP/1.0 500 Internal Server Error\r\n',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 131, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 640, in send
    history = [resp for resp in gen] if allow_redirects else []
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 640, in <listcomp>
    history = [resp for resp in gen] if allow_redirects else []
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 218, in resolve_redirects
    **adapter_kwargs
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 490, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', BadStatusLine('\x15\x03\x01\x00\x02\x02\x16HTTP/1.0 500 Internal Server Error\r\n',))

2018- 9-09 20:09:55 crawl_class.py[line:29] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x0000017D66ED00F0>, 'Connection to 139.219.225.161 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='139.219.225.161', port=8081): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x0000017D66ED00F0>, 'Connection to 139.219.225.161 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 131, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='139.219.225.161', port=8081): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x0000017D66ED00F0>, 'Connection to 139.219.225.161 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 27, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 188, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 195, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:12:10 crawl_class.py[line:29] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x00000227BC1510B8>, 'Connection to 222.222.243.124 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='222.222.243.124', port=8060): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000227BC1510B8>, 'Connection to 222.222.243.124 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 131, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='222.222.243.124', port=8060): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000227BC1510B8>, 'Connection to 222.222.243.124 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 27, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 188, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 195, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:12:15 crawl_class.py[line:29] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x00000227BC151F60>, 'Connection to 110.73.51.55 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='110.73.51.55', port=8123): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000227BC151F60>, 'Connection to 110.73.51.55 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 131, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='110.73.51.55', port=8123): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000227BC151F60>, 'Connection to 110.73.51.55 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 27, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 188, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 195, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:12:33 crawl_class.py[line:150] ERROR Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 595, in urlopen
    self._prepare_proxy(conn)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 816, in _prepare_proxy
    conn.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 294, in connect
    self._tunnel()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 804, in _tunnel
    (version, code, message) = response._read_status()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 264, in _read_status
    raise BadStatusLine(line)
http.client.BadStatusLine: <html>


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 357, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\packages\six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 595, in urlopen
    self._prepare_proxy(conn)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 816, in _prepare_proxy
    conn.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 294, in connect
    self._tunnel()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 804, in _tunnel
    (version, code, message) = response._read_status()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 264, in _read_status
    raise BadStatusLine(line)
urllib3.exceptions.ProtocolError: ('Connection aborted.', BadStatusLine('<html>\r\n',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 131, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 640, in send
    history = [resp for resp in gen] if allow_redirects else []
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 640, in <listcomp>
    history = [resp for resp in gen] if allow_redirects else []
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 218, in resolve_redirects
    **adapter_kwargs
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 490, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', BadStatusLine('<html>\r\n',))

2018- 9-09 20:12:33 crawl_class.py[line:151] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:12:40 crawl_class.py[line:29] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x00000227BC1C85F8>, 'Connection to 114.99.26.182 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='114.99.26.182', port=18118): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000227BC1C85F8>, 'Connection to 114.99.26.182 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 131, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='114.99.26.182', port=18118): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000227BC1C85F8>, 'Connection to 114.99.26.182 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 27, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 188, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 195, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:12:45 crawl_class.py[line:29] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x00000227BC1C8EB8>, 'Connection to 61.50.107.98 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='61.50.107.98', port=8060): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000227BC1C8EB8>, 'Connection to 61.50.107.98 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 131, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='61.50.107.98', port=8060): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000227BC1C8EB8>, 'Connection to 61.50.107.98 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 27, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 188, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 195, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:13:00 crawl_class.py[line:150] ERROR Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\response.py", line 543, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\response.py", line 302, in _error_catcher
    yield
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\response.py", line 598, in read_chunked
    self._update_chunk_length()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\response.py", line 547, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\models.py", line 745, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\response.py", line 432, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\response.py", line 626, in read_chunked
    self._original_response.close()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\response.py", line 320, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 131, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 658, in send
    r.content
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\models.py", line 823, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\models.py", line 748, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 20:13:01 crawl_class.py[line:151] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:13:09 crawl_class.py[line:29] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x00000227BC1E6710>, 'Connection to 218.244.44.194 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='218.244.44.194', port=8060): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000227BC1E6710>, 'Connection to 218.244.44.194 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 131, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='218.244.44.194', port=8060): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x00000227BC1E6710>, 'Connection to 218.244.44.194 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 27, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 188, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 195, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:13:17 crawl_class.py[line:29] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 133, in crawl
    data = parse_func(response, url)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 205, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 27, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 188, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 195, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:14:15 crawl_class.py[line:150] ERROR Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 150, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001A3420C20B8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='183.159.89.201', port=18118): Max retries exceeded with url: http://www.baidu.com/ (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001A3420C20B8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 131, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 502, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPConnectionPool(host='183.159.89.201', port=18118): Max retries exceeded with url: http://www.baidu.com/ (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001A3420C20B8>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',)))

2018- 9-09 20:14:16 crawl_class.py[line:151] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:16:30 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x000002363E3200F0>, 'Connection to 114.113.126.86 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='114.113.126.86', port=80): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002363E3200F0>, 'Connection to 114.113.126.86 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 132, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='114.113.126.86', port=80): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002363E3200F0>, 'Connection to 114.113.126.86 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 191, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 198, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:16:35 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x000002363E320F98>, 'Connection to 218.72.67.49 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='218.72.67.49', port=18118): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002363E320F98>, 'Connection to 218.72.67.49 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 132, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='218.72.67.49', port=18118): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002363E320F98>, 'Connection to 218.72.67.49 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 191, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 198, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:16:39 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x000002363E390780>, 'Connection to 222.33.192.238 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='222.33.192.238', port=8118): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002363E390780>, 'Connection to 222.33.192.238 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 132, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='222.33.192.238', port=8118): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002363E390780>, 'Connection to 222.33.192.238 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 191, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 198, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:16:46 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 134, in crawl
    data = parse_func(response, url)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 208, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 191, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 198, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:16:50 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x000002363E390588>, 'Connection to 122.137.83.134 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='122.137.83.134', port=8060): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002363E390588>, 'Connection to 122.137.83.134 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 132, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='122.137.83.134', port=8060): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002363E390588>, 'Connection to 122.137.83.134 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 191, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 198, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:18:13 crawl_class.py[line:153] ERROR Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 387, in _make_request
    six.raise_from(e, None)
  File "<string>", line 2, in raise_from
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 383, in _make_request
    httplib_response = conn.getresponse()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1174, in getresponse
    response.begin()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 282, in begin
    version, status, reason = self._read_status()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 243, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\socket.py", line 571, in readinto
    return self._sock.recv_into(b)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 357, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 389, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 309, in _raise_timeout
    raise ReadTimeoutError(self, url, "Read timed out. (read timeout=%s)" % timeout_value)
urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='221.10.159.234', port=1337): Read timed out. (read timeout=2)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 132, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 521, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: HTTPConnectionPool(host='221.10.159.234', port=1337): Read timed out. (read timeout=2)

2018- 9-09 20:18:14 crawl_class.py[line:154] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 20:18:22 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x000002363E399DA0>, 'Connection to 39.107.204.193 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='39.107.204.193', port=8088): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002363E399DA0>, 'Connection to 39.107.204.193 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 132, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='39.107.204.193', port=8088): Max retries exceeded with url: http://www.baidu.com/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000002363E399DA0>, 'Connection to 39.107.204.193 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 191, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 198, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:19:26 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 135, in crawl
    data = parse_func(response, url)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 209, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 192, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 199, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:19:32 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x000001F171EA0898>, 'Connection to 223.241.78.67 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='223.241.78.67', port=18118): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000001F171EA0898>, 'Connection to 223.241.78.67 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='223.241.78.67', port=18118): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000001F171EA0898>, 'Connection to 223.241.78.67 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 192, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 199, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:19:38 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x000001F171F08748>, 'Connection to 218.72.67.49 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='218.72.67.49', port=18118): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000001F171F08748>, 'Connection to 218.72.67.49 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='218.72.67.49', port=18118): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000001F171F08748>, 'Connection to 218.72.67.49 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 192, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 199, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:20:26 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 135, in crawl
    data = parse_func(response, url)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 209, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 192, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 199, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:20:36 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x000001486FCA00F0>, 'Connection to 113.253.113.90 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='113.253.113.90', port=8380): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000001486FCA00F0>, 'Connection to 113.253.113.90 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='113.253.113.90', port=8380): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000001486FCA00F0>, 'Connection to 113.253.113.90 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 192, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 199, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:20:51 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 150, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001486FCA0F98>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='218.72.110.203', port=18118): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001486FCA0F98>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 502, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPConnectionPool(host='218.72.110.203', port=18118): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001486FCA0F98>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 192, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 199, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:20:57 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x000001486FD0F780>, 'Connection to 118.24.22.152 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='118.24.22.152', port=3128): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000001486FD0F780>, 'Connection to 118.24.22.152 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='118.24.22.152', port=3128): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000001486FD0F780>, 'Connection to 118.24.22.152 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 192, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 199, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:21:02 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x000001486FD0FF60>, 'Connection to 125.75.124.219 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='125.75.124.219', port=8060): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000001486FD0FF60>, 'Connection to 125.75.124.219 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='125.75.124.219', port=8060): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000001486FD0FF60>, 'Connection to 125.75.124.219 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 192, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 199, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:21:07 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x000001486FD0FA20>, 'Connection to 221.221.138.166 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='221.221.138.166', port=8060): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000001486FD0FA20>, 'Connection to 221.221.138.166 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='221.221.138.166', port=8060): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000001486FD0FA20>, 'Connection to 221.221.138.166 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 192, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 199, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:21:15 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 150, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001486FD0F240>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='117.64.236.127', port=18118): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001486FD0F240>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 502, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPConnectionPool(host='117.64.236.127', port=18118): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001486FD0F240>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 192, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 199, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:21:20 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x000001486FCA0C18>, 'Connection to 200.255.122.174 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='200.255.122.174', port=8080): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000001486FCA0C18>, 'Connection to 200.255.122.174 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='200.255.122.174', port=8080): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000001486FCA0C18>, 'Connection to 200.255.122.174 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 192, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 199, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:21:25 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 150, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001486FD1A860>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='183.159.90.60', port=18118): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001486FD1A860>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 502, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPConnectionPool(host='183.159.90.60', port=18118): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001486FD1A860>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 192, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 199, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:21:31 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 387, in _make_request
    six.raise_from(e, None)
  File "<string>", line 2, in raise_from
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 383, in _make_request
    httplib_response = conn.getresponse()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1174, in getresponse
    response.begin()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 282, in begin
    version, status, reason = self._read_status()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 243, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\socket.py", line 571, in readinto
    return self._sock.recv_into(b)
ConnectionAbortedError: [WinError 10053] 你的主机中的软件中止了一个已建立的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='59.63.53.123', port=61234): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 502, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPConnectionPool(host='59.63.53.123', port=61234): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 192, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 199, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:21:36 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 387, in _make_request
    six.raise_from(e, None)
  File "<string>", line 2, in raise_from
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 383, in _make_request
    httplib_response = conn.getresponse()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1174, in getresponse
    response.begin()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 282, in begin
    version, status, reason = self._read_status()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 243, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\socket.py", line 571, in readinto
    return self._sock.recv_into(b)
ConnectionAbortedError: [WinError 10053] 你的主机中的软件中止了一个已建立的连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='119.179.210.183', port=61234): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 502, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPConnectionPool(host='119.179.210.183', port=61234): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 192, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 199, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:22:29 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 150, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x0000024CE1D6EF28>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='61.178.49.52', port=8908): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024CE1D6EF28>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 502, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPConnectionPool(host='61.178.49.52', port=8908): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024CE1D6EF28>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 194, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 201, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:22:31 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x0000024CE1DCDEF0>, 'Connection to 218.72.111.50 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='218.72.111.50', port=18118): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x0000024CE1DCDEF0>, 'Connection to 218.72.111.50 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='218.72.111.50', port=18118): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x0000024CE1DCDEF0>, 'Connection to 218.72.111.50 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 194, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 201, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:22:31 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 387, in _make_request
    six.raise_from(e, None)
  File "<string>", line 2, in raise_from
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 383, in _make_request
    httplib_response = conn.getresponse()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1174, in getresponse
    response.begin()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 282, in begin
    version, status, reason = self._read_status()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 251, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='223.245.241.154', port=61234): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 502, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPConnectionPool(host='223.245.241.154', port=61234): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 194, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 201, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:23:00 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x000001468695B6A0>, 'Connection to 223.241.119.183 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='223.241.119.183', port=18118): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000001468695B6A0>, 'Connection to 223.241.119.183 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='223.241.119.183', port=18118): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000001468695B6A0>, 'Connection to 223.241.119.183 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 194, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 201, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:23:01 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 150, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001468699F828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='60.177.230.13', port=18118): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001468699F828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 502, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPConnectionPool(host='60.177.230.13', port=18118): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001468699F828>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 194, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 201, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:23:20 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x000001B3CFAFFC50>, 'Connection to 117.68.194.159 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='117.68.194.159', port=18118): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000001B3CFAFFC50>, 'Connection to 117.68.194.159 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='117.68.194.159', port=18118): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000001B3CFAFFC50>, 'Connection to 117.68.194.159 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 194, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 201, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:23:20 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 387, in _make_request
    six.raise_from(e, None)
  File "<string>", line 2, in raise_from
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 383, in _make_request
    httplib_response = conn.getresponse()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1174, in getresponse
    response.begin()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 282, in begin
    version, status, reason = self._read_status()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 251, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='182.34.16.172', port=61234): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 502, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPConnectionPool(host='182.34.16.172', port=61234): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 194, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 201, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:23:23 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 387, in _make_request
    six.raise_from(e, None)
  File "<string>", line 2, in raise_from
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 383, in _make_request
    httplib_response = conn.getresponse()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1174, in getresponse
    response.begin()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 282, in begin
    version, status, reason = self._read_status()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 243, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\socket.py", line 571, in readinto
    return self._sock.recv_into(b)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 357, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\packages\six.py", line 686, in reraise
    raise value
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 389, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 309, in _raise_timeout
    raise ReadTimeoutError(self, url, "Read timed out. (read timeout=%s)" % timeout_value)
urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='122.156.230.45', port=8908): Read timed out. (read timeout=2)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 521, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: HTTPConnectionPool(host='122.156.230.45', port=8908): Read timed out. (read timeout=2)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 194, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 201, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 20:23:25 crawl_class.py[line:30] ERROR 请求ip池接口报错>>>>Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 83, in create_connection
    raise err
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1083, in request
    self._send_request(method, url, body, headers)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1128, in _send_request
    self.endheaders(body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 1079, in endheaders
    self._send_output(message_body)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 911, in _send_output
    self.send(msg)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\http\client.py", line 854, in send
    self.connect()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 166, in connect
    conn = self._new_conn()
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x000001B3CFB12E48>, 'Connection to 139.219.225.161 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 440, in send
    timeout=timeout
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\urllib3\util\retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='139.219.225.161', port=8081): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000001B3CFB12E48>, 'Connection to 139.219.225.161 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "C:\Users\namibox\AppData\Local\Programs\Python\Python35\lib\site-packages\requests\adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='139.219.225.161', port=8081): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x000001B3CFB12E48>, 'Connection to 139.219.225.161 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 194, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "C:/Users/namibox/Desktop/自程序/藏宝阁/cbg_backup/crawl_url/crawl/crawl_class.py", line 201, in handle_with_hang_ip
    self.proxy_list.remove(proxy['ip'])
ValueError: list.remove(x): x not in list

2018- 9-09 21:02:42 crawl_class.py[line:157] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 21:02:55 crawl_class.py[line:157] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 21:02:59 crawl_class.py[line:157] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 21:03:13 crawl_class.py[line:157] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 21:03:13 crawl_class.py[line:157] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 21:03:16 crawl_class.py[line:157] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 21:03:17 crawl_class.py[line:157] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 21:03:19 crawl_class.py[line:157] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 21:05:03 crawl_class.py[line:157] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 21:05:04 crawl_class.py[line:157] ERROR 发生错误，%s订单%s url % 页已停止爬取
2018- 9-09 22:10:20 crawl_class.py[line:157] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 377, in _make_request
    httplib_response = conn.getresponse(buffering=True)
TypeError: getresponse() got an unexpected keyword argument 'buffering'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 560, in urlopen
    body=body, headers=headers)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 379, in _make_request
    httplib_response = conn.getresponse()
  File "/usr/lib/python3.5/http/client.py", line 1197, in getresponse
    response.begin()
  File "/usr/lib/python3.5/http/client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "/usr/lib/python3.5/http/client.py", line 266, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='39.134.10.14', port=8080): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='39.134.10.14', port=8080): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

2018- 9-09 22:10:20 crawl_class.py[line:158] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:10:28 crawl_class.py[line:157] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 377, in _make_request
    httplib_response = conn.getresponse(buffering=True)
TypeError: getresponse() got an unexpected keyword argument 'buffering'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 560, in urlopen
    body=body, headers=headers)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 379, in _make_request
    httplib_response = conn.getresponse()
  File "/usr/lib/python3.5/http/client.py", line 1197, in getresponse
    response.begin()
  File "/usr/lib/python3.5/http/client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='39.134.10.10', port=8080): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='39.134.10.10', port=8080): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

2018- 9-09 22:10:28 crawl_class.py[line:158] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:10:28 crawl_class.py[line:157] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 377, in _make_request
    httplib_response = conn.getresponse(buffering=True)
TypeError: getresponse() got an unexpected keyword argument 'buffering'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 560, in urlopen
    body=body, headers=headers)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 379, in _make_request
    httplib_response = conn.getresponse()
  File "/usr/lib/python3.5/http/client.py", line 1197, in getresponse
    response.begin()
  File "/usr/lib/python3.5/http/client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='39.134.10.10', port=8080): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='39.134.10.10', port=8080): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

2018- 9-09 22:10:28 crawl_class.py[line:158] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:10:32 crawl_class.py[line:157] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 377, in _make_request
    httplib_response = conn.getresponse(buffering=True)
TypeError: getresponse() got an unexpected keyword argument 'buffering'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 560, in urlopen
    body=body, headers=headers)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 379, in _make_request
    httplib_response = conn.getresponse()
  File "/usr/lib/python3.5/http/client.py", line 1197, in getresponse
    response.begin()
  File "/usr/lib/python3.5/http/client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "/usr/lib/python3.5/http/client.py", line 266, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='39.134.10.11', port=8080): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='39.134.10.11', port=8080): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

2018- 9-09 22:10:32 crawl_class.py[line:158] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:10:35 crawl_class.py[line:157] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 377, in _make_request
    httplib_response = conn.getresponse(buffering=True)
TypeError: getresponse() got an unexpected keyword argument 'buffering'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 560, in urlopen
    body=body, headers=headers)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 379, in _make_request
    httplib_response = conn.getresponse()
  File "/usr/lib/python3.5/http/client.py", line 1197, in getresponse
    response.begin()
  File "/usr/lib/python3.5/http/client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "/usr/lib/python3.5/http/client.py", line 266, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='39.134.10.11', port=8080): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='39.134.10.11', port=8080): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

2018- 9-09 22:10:35 crawl_class.py[line:158] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:10:35 crawl_class.py[line:157] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 377, in _make_request
    httplib_response = conn.getresponse(buffering=True)
TypeError: getresponse() got an unexpected keyword argument 'buffering'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 560, in urlopen
    body=body, headers=headers)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 379, in _make_request
    httplib_response = conn.getresponse()
  File "/usr/lib/python3.5/http/client.py", line 1197, in getresponse
    response.begin()
  File "/usr/lib/python3.5/http/client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "/usr/lib/python3.5/http/client.py", line 266, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='39.134.10.11', port=8080): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='39.134.10.11', port=8080): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

2018- 9-09 22:10:35 crawl_class.py[line:158] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:10:36 crawl_class.py[line:157] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 377, in _make_request
    httplib_response = conn.getresponse(buffering=True)
TypeError: getresponse() got an unexpected keyword argument 'buffering'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 560, in urlopen
    body=body, headers=headers)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 379, in _make_request
    httplib_response = conn.getresponse()
  File "/usr/lib/python3.5/http/client.py", line 1197, in getresponse
    response.begin()
  File "/usr/lib/python3.5/http/client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "/usr/lib/python3.5/http/client.py", line 266, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='39.134.10.14', port=8080): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='39.134.10.14', port=8080): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

2018- 9-09 22:10:36 crawl_class.py[line:158] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:10:36 crawl_class.py[line:157] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 377, in _make_request
    httplib_response = conn.getresponse(buffering=True)
TypeError: getresponse() got an unexpected keyword argument 'buffering'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 560, in urlopen
    body=body, headers=headers)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 379, in _make_request
    httplib_response = conn.getresponse()
  File "/usr/lib/python3.5/http/client.py", line 1197, in getresponse
    response.begin()
  File "/usr/lib/python3.5/http/client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "/usr/lib/python3.5/http/client.py", line 266, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='39.134.10.11', port=8080): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='39.134.10.11', port=8080): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

2018- 9-09 22:10:36 crawl_class.py[line:158] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:10:38 crawl_class.py[line:157] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 377, in _make_request
    httplib_response = conn.getresponse(buffering=True)
TypeError: getresponse() got an unexpected keyword argument 'buffering'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 560, in urlopen
    body=body, headers=headers)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 379, in _make_request
    httplib_response = conn.getresponse()
  File "/usr/lib/python3.5/http/client.py", line 1197, in getresponse
    response.begin()
  File "/usr/lib/python3.5/http/client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "/usr/lib/python3.5/http/client.py", line 266, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='39.134.10.10', port=8080): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='39.134.10.10', port=8080): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

2018- 9-09 22:10:38 crawl_class.py[line:158] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:10:40 crawl_class.py[line:157] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 377, in _make_request
    httplib_response = conn.getresponse(buffering=True)
TypeError: getresponse() got an unexpected keyword argument 'buffering'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 560, in urlopen
    body=body, headers=headers)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 379, in _make_request
    httplib_response = conn.getresponse()
  File "/usr/lib/python3.5/http/client.py", line 1197, in getresponse
    response.begin()
  File "/usr/lib/python3.5/http/client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "/usr/lib/python3.5/http/client.py", line 258, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='39.134.10.10', port=8080): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='39.134.10.10', port=8080): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', ConnectionResetError(104, 'Connection reset by peer')))

2018- 9-09 22:10:40 crawl_class.py[line:158] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:10:40 crawl_class.py[line:157] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 377, in _make_request
    httplib_response = conn.getresponse(buffering=True)
TypeError: getresponse() got an unexpected keyword argument 'buffering'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 560, in urlopen
    body=body, headers=headers)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 379, in _make_request
    httplib_response = conn.getresponse()
  File "/usr/lib/python3.5/http/client.py", line 1197, in getresponse
    response.begin()
  File "/usr/lib/python3.5/http/client.py", line 297, in begin
    version, status, reason = self._read_status()
  File "/usr/lib/python3.5/http/client.py", line 266, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 376, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 610, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='39.134.10.11', port=8080): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 576, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 437, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='39.134.10.11', port=8080): Max retries exceeded with url: http://www.guwenjiang.com/cbg/role (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response',)))

2018- 9-09 22:10:40 crawl_class.py[line:158] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:12:58 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:12:58 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:12:59 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:12:59 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:13:00 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:13:00 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:13:01 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:13:01 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:13:02 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:13:02 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:13:03 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:13:03 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:14:20 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:14:20 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:14:38 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:14:38 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:14:55 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:14:55 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:14:56 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:14:56 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:14:59 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:14:59 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:01 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:01 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:01 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:01 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:02 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:02 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:02 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:02 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:06 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:06 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:06 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:06 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:10 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:10 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:12 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:12 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:12 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:12 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:14 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:14 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:15 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:15 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:17 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:17 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:18 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:18 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:19 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:19 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:20 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:20 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:27 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:27 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:27 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:27 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:31 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:31 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:34 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:34 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:36 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:36 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:38 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:38 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:41 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:41 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:42 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:42 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:44 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:44 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:48 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:48 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:49 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:49 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:50 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:50 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:50 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:50 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:51 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:51 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:52 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:52 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:53 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:53 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:54 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:54 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:55 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:55 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:57 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:57 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:15:58 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:15:58 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:16:05 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:16:05 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:16:08 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:16:08 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:16:16 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:16:16 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:16:18 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:16:18 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:16:19 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:16:19 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:16:19 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:16:19 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:16:21 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:16:21 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:16:23 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:16:23 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:16:23 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:16:23 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:16:24 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:16:24 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:16:25 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:16:25 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:16:25 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:16:25 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:16:26 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:16:26 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:16:26 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:16:26 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:16:27 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:16:27 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:16:27 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:16:27 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:16:27 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:16:27 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:16:30 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:16:30 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:16:35 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:16:35 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:16:36 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:16:36 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:16:39 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:16:39 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:16:39 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:16:39 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:16:44 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:16:44 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:16:45 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:16:45 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:16:45 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:16:45 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:16:47 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:16:47 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:16:47 crawl_class.py[line:159] ERROR Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 435, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 226, in _error_catcher
    yield
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 486, in read_chunked
    self._update_chunk_length()
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 439, in _update_chunk_length
    raise httplib.IncompleteRead(line)
http.client.IncompleteRead: IncompleteRead(0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/models.py", line 660, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 340, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 514, in read_chunked
    self._original_response.close()
  File "/usr/lib/python3.5/contextlib.py", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/lib/python3/dist-packages/urllib3/response.py", line 244, in _error_catcher
    raise ProtocolError('Connection broken: %r' % e, e)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 133, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 67, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 53, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 608, in send
    r.content
  File "/usr/lib/python3/dist-packages/requests/models.py", line 737, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/usr/lib/python3/dist-packages/requests/models.py", line 663, in generate
    raise ChunkedEncodingError(e)
requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))

2018- 9-09 22:16:47 crawl_class.py[line:160] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:28:41 crawl_class.py[line:130] WARNING 璇娆℃拌揪颁 宸茬姝http://www.guwenjiang.com/cbg/role>>>>>>
2018- 9-09 22:28:55 crawl_class.py[line:130] WARNING 璇娆℃拌揪颁 宸茬姝http://www.guwenjiang.com/cbg/role>>>>>>
2018- 9-09 22:29:06 crawl_class.py[line:130] WARNING 璇娆℃拌揪颁 宸茬姝http://www.guwenjiang.com/cbg/role>>>>>>
2018- 9-09 22:29:18 crawl_class.py[line:130] WARNING 璇娆℃拌揪颁 宸茬姝http://www.guwenjiang.com/cbg/role>>>>>>
2018- 9-09 22:29:31 crawl_class.py[line:130] WARNING 璇娆℃拌揪颁 宸茬姝http://www.guwenjiang.com/cbg/role>>>>>>
2018- 9-09 22:29:43 crawl_class.py[line:130] WARNING 璇娆℃拌揪颁 宸茬姝http://www.guwenjiang.com/cbg/role>>>>>>
2018- 9-09 22:29:57 crawl_class.py[line:130] WARNING 璇娆℃拌揪颁 宸茬姝http://www.guwenjiang.com/cbg/role>>>>>>
2018- 9-09 22:30:07 crawl_class.py[line:130] WARNING 璇娆℃拌揪颁 宸茬姝http://www.guwenjiang.com/cbg/role>>>>>>
2018- 9-09 22:31:00 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:00 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:00 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:00 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:00 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:00 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:00 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:00 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:00 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:00 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:00 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:00 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:00 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:00 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:00 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:00 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:00 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:00 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:00 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:00 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:00 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:00 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:00 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:00 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:00 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:00 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:00 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:00 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:00 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:00 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:01 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:01 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:31:02 crawl_class.py[line:162] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 136, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 193, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:31:02 crawl_class.py[line:163] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:20 crawl_class.py[line:132] WARNING 璇娆℃拌揪颁 宸茬姝http://www.baidu.com>>>>>>
2018- 9-09 22:49:37 crawl_class.py[line:132] WARNING 璇娆℃拌揪颁 宸茬姝http://www.baidu.com>>>>>>
2018- 9-09 22:49:56 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:56 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:56 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:56 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:56 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:56 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:56 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:56 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:56 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:56 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:56 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:56 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:56 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:56 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:56 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:56 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:56 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:56 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:56 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:56 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:56 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:56 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:56 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:56 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:56 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:56 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:56 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:56 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:56 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:56 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:56 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:56 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:56 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:56 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:56 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:56 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:56 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:56 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:56 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:56 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:56 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:56 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:56 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:56 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:57 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:57 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:49:58 crawl_class.py[line:163] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    self.get_proxy_list()
  File "crawl_class.py", line 194, in get_proxy_list
    proxy_list = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 22:49:58 crawl_class.py[line:164] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 22:54:43 crawl_class.py[line:132] WARNING 璇娆℃拌揪颁 宸茬姝http://www.baidu.com>>>>>>
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:16:27 crawl_class.py[line:183] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 137, in crawl
    if self.proxy_list <= self.limit_proxy_count:  # 褰缁存ょ浠ｇ姹灏浜20涓讹 瑕扮浠ｇ
TypeError: unorderable types: list() <= int()

2018- 9-09 23:16:27 crawl_class.py[line:184] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:20:27 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:20:27 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:20:27 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:20:27 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:20:27 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:20:27 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:21:42 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:21:42 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:21:42 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:21:42 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:21:42 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:21:42 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:21:43 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:21:43 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:21:46 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:21:46 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:21:46 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:21:46 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:21:59 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:21:59 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:21:59 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:21:59 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:22:00 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:22:00 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:22:01 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:22:01 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:22:03 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:22:03 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:22:09 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:22:09 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:22:10 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:22:10 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:22:19 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:22:19 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:22:19 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:22:19 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:22:19 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:22:19 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:22:19 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:22:19 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:22:20 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:22:20 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:22:20 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:22:20 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:22:23 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:22:23 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:22:23 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:22:23 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:22:23 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:22:23 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:22:26 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:22:26 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:22:29 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:22:29 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:22:32 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:22:32 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:22:32 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:22:32 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:22:32 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:22:32 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:22:32 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:22:32 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:22:33 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:22:33 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:22:36 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:22:36 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:22:37 crawl_class.py[line:176] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:22:37 crawl_class.py[line:177] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:23:31 crawl_class.py[line:177] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 250, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:23:31 crawl_class.py[line:178] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:23:33 crawl_class.py[line:177] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 250, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:23:33 crawl_class.py[line:178] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:24:18 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:24:22 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:24:38 crawl_class.py[line:134] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role?page=1>>>>>>
2018- 9-09 23:25:06 crawl_class.py[line:134] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role?page=2>>>>>>
2018- 9-09 23:25:55 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:26:01 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:26:17 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:27:04 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:27:08 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:27:13 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:27:14 crawl_class.py[line:134] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=1>>>>>>
2018- 9-09 23:27:14 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:27:41 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:27:51 crawl_class.py[line:134] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=2>>>>>>
2018- 9-09 23:28:20 crawl_class.py[line:134] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=3>>>>>>
2018- 9-09 23:28:33 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:28:39 crawl_class.py[line:134] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=4>>>>>>
2018- 9-09 23:28:40 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:28:44 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:28:44 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:28:45 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:28:47 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:28:47 crawl_class.py[line:134] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=5>>>>>>
2018- 9-09 23:28:53 crawl_class.py[line:134] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=6>>>>>>
2018- 9-09 23:28:54 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:28:59 crawl_class.py[line:134] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=7>>>>>>
2018- 9-09 23:29:01 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:29:04 crawl_class.py[line:134] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=8>>>>>>
2018- 9-09 23:29:06 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:29:08 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:29:10 crawl_class.py[line:134] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=9>>>>>>
2018- 9-09 23:29:11 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:29:12 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:29:14 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:29:14 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:29:15 crawl_class.py[line:134] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=10>>>>>>
2018- 9-09 23:29:20 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:29:20 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:29:20 crawl_class.py[line:134] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=11>>>>>>
2018- 9-09 23:29:21 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:29:24 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:29:25 crawl_class.py[line:134] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=12>>>>>>
2018- 9-09 23:29:25 crawl_class.py[line:30] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 249, in parse
    raise NotTarget
NotTarget

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 28, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 226, in handle_with_dity_ip
    self.handle_with_hang_ip(proxy)
  File "crawl_class.py", line 236, in handle_with_hang_ip
    print('や%s, 杩%s涓浠ｇ' % (proxy['ip'], len(self.proxy_list)))
KeyError: 'ip'

2018- 9-09 23:40:21 crawl_class.py[line:180] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 150, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 253, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:40:21 crawl_class.py[line:181] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:40:40 crawl_class.py[line:180] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 150, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 253, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:40:40 crawl_class.py[line:181] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:41:02 crawl_class.py[line:180] ERROR Traceback (most recent call last):
  File "crawl_class.py", line 150, in crawl
    data = parse_func(response, url)
  File "crawl_class.py", line 253, in parse
    data = json.loads(response.text)
  File "/usr/lib/python3.5/json/__init__.py", line 319, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.5/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.5/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

2018- 9-09 23:41:02 crawl_class.py[line:181] ERROR 璇锛%s璁㈠%s url % 椤靛凡姝㈢
2018- 9-09 23:46:34 crawl_class.py[line:32] ERROR 璇锋ip姹ュｆラ>>>>Traceback (most recent call last):
  File "/home/ubuntu/.virtualenvs/django-2/lib/python3.5/site-packages/urllib3/connection.py", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "/home/ubuntu/.virtualenvs/django-2/lib/python3.5/site-packages/urllib3/util/connection.py", line 83, in create_connection
    raise err
  File "/home/ubuntu/.virtualenvs/django-2/lib/python3.5/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.virtualenvs/django-2/lib/python3.5/site-packages/urllib3/connectionpool.py", line 601, in urlopen
    chunked=chunked)
  File "/home/ubuntu/.virtualenvs/django-2/lib/python3.5/site-packages/urllib3/connectionpool.py", line 357, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/usr/lib/python3.5/http/client.py", line 1106, in request
    self._send_request(method, url, body, headers)
  File "/usr/lib/python3.5/http/client.py", line 1151, in _send_request
    self.endheaders(body)
  File "/usr/lib/python3.5/http/client.py", line 1102, in endheaders
    self._send_output(message_body)
  File "/usr/lib/python3.5/http/client.py", line 934, in _send_output
    self.send(msg)
  File "/usr/lib/python3.5/http/client.py", line 877, in send
    self.connect()
  File "/home/ubuntu/.virtualenvs/django-2/lib/python3.5/site-packages/urllib3/connection.py", line 166, in connect
    conn = self._new_conn()
  File "/home/ubuntu/.virtualenvs/django-2/lib/python3.5/site-packages/urllib3/connection.py", line 146, in _new_conn
    (self.host, self.timeout))
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x7fda035764a8>, 'Connection to 117.64.237.119 timed out. (connect timeout=2)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/.virtualenvs/django-2/lib/python3.5/site-packages/requests/adapters.py", line 440, in send
    timeout=timeout
  File "/home/ubuntu/.virtualenvs/django-2/lib/python3.5/site-packages/urllib3/connectionpool.py", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/home/ubuntu/.virtualenvs/django-2/lib/python3.5/site-packages/urllib3/util/retry.py", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='117.64.237.119', port=18118): Max retries exceeded with url: http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7fda035764a8>, 'Connection to 117.64.237.119 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 147, in crawl
    response = requests.get(url, proxies=proxy, headers=header, timeout=2)
  File "/home/ubuntu/.virtualenvs/django-2/lib/python3.5/site-packages/requests/api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "/home/ubuntu/.virtualenvs/django-2/lib/python3.5/site-packages/requests/api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/ubuntu/.virtualenvs/django-2/lib/python3.5/site-packages/requests/sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/ubuntu/.virtualenvs/django-2/lib/python3.5/site-packages/requests/sessions.py", line 618, in send
    r = adapter.send(request, **kwargs)
  File "/home/ubuntu/.virtualenvs/django-2/lib/python3.5/site-packages/requests/adapters.py", line 496, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPConnectionPool(host='117.64.237.119', port=18118): Max retries exceeded with url: http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7fda035764a8>, 'Connection to 117.64.237.119 timed out. (connect timeout=2)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "crawl_class.py", line 30, in wrapper
    return func(*args, **kwargs)
  File "crawl_class.py", line 231, in handle_with_dity_ip
    requests.get(self.ip_api_delete.format(proxy['ip']))
  File "/home/ubuntu/.virtualenvs/django-2/lib/python3.5/site-packages/requests/api.py", line 72, in get
    return request('get', url, params=params, **kwargs)
  File "/home/ubuntu/.virtualenvs/django-2/lib/python3.5/site-packages/requests/api.py", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/ubuntu/.virtualenvs/django-2/lib/python3.5/site-packages/requests/sessions.py", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/ubuntu/.virtualenvs/django-2/lib/python3.5/site-packages/requests/sessions.py", line 658, in send
    r.content
  File "/home/ubuntu/.virtualenvs/django-2/lib/python3.5/site-packages/requests/models.py", line 823, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File "/home/ubuntu/.virtualenvs/django-2/lib/python3.5/site-packages/requests/models.py", line 745, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File "/home/ubuntu/.virtualenvs/django-2/lib/python3.5/site-packages/urllib3/response.py", line 432, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File "/home/ubuntu/.virtualenvs/django-2/lib/python3.5/site-packages/urllib3/response.py", line 598, in read_chunked
    self._update_chunk_length()
  File "/home/ubuntu/.virtualenvs/django-2/lib/python3.5/site-packages/urllib3/response.py", line 540, in _update_chunk_length
    line = self._fp.fp.readline()
  File "/usr/lib/python3.5/socket.py", line 575, in readinto
    return self._sock.recv_into(b)
KeyboardInterrupt

2018- 9-09 23:50:02 crawl_class.py[line:138] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=32>>>>>>
2018- 9-09 23:56:36 crawl_class.py[line:142] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=2>>>>>>
2018- 9-09 23:57:06 crawl_class.py[line:142] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=12>>>>>>
2018- 9-09 23:57:19 crawl_class.py[line:142] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=19>>>>>>
2018- 9-09 23:57:27 crawl_class.py[line:142] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=23>>>>>>
2018- 9-09 23:57:37 crawl_class.py[line:142] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=29>>>>>>
2018- 9-09 23:58:03 crawl_class.py[line:263] WARNING 版缂哄け>>>
2018-10-10 08:41:23 crawl_class.py[line:291] WARNING 版缂哄け>>>
2018-10-10 08:42:02 crawl_class.py[line:291] WARNING 版缂哄け>>>
2018-10-10 08:45:26 crawl_class.py[line:291] WARNING 版缂哄け>>>
2018-10-10 08:54:39 crawl_class.py[line:293] WARNING 版缂哄け>>>
2018-10-10 08:57:28 crawl_class.py[line:159] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=10>>>>>>
2018-10-10 08:57:39 crawl_class.py[line:293] WARNING 版缂哄け>>>
2018-10-10 08:58:54 crawl_class.py[line:294] WARNING 版缂哄け>>>
2018-10-10 09:01:02 crawl_class.py[line:159] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=33>>>>>>
2018-10-10 09:07:03 crawl_class.py[line:295] WARNING 版缂哄け>>>
2018-10-10 09:14:48 crawl_class.py[line:304] WARNING 版缂哄け>>>
2018-10-10 09:16:07 crawl_class.py[line:304] WARNING 版缂哄け>>>
2018-10-10 09:16:20 crawl_class.py[line:304] WARNING 版缂哄け>>>
2018-10-10 09:16:23 crawl_class.py[line:304] WARNING 版缂哄け>>>
2018-10-10 09:16:39 crawl_class.py[line:304] WARNING 版缂哄け>>>
2018-10-10 09:16:55 crawl_class.py[line:304] WARNING 版缂哄け>>>
2018-10-10 09:16:57 crawl_class.py[line:304] WARNING 版缂哄け>>>
2018-10-10 09:17:00 crawl_class.py[line:304] WARNING 版缂哄け>>>
2018-10-10 09:17:01 crawl_class.py[line:304] WARNING 版缂哄け>>>
2018-10-10 09:17:05 crawl_class.py[line:304] WARNING 版缂哄け>>>
2018-10-10 09:17:08 crawl_class.py[line:304] WARNING 版缂哄け>>>
2018-10-10 09:17:08 crawl_class.py[line:304] WARNING 版缂哄け>>>
2018-10-10 09:17:09 crawl_class.py[line:304] WARNING 版缂哄け>>>
2018-10-10 09:17:11 crawl_class.py[line:304] WARNING 版缂哄け>>>
2018-10-10 09:17:13 crawl_class.py[line:304] WARNING 版缂哄け>>>
2018-10-10 09:17:16 crawl_class.py[line:304] WARNING 版缂哄け>>>
2018-10-10 09:17:40 crawl_class.py[line:304] WARNING 版缂哄け>>>
2018-10-10 09:17:51 crawl_class.py[line:304] WARNING 版缂哄け>>>
2018-10-10 09:18:03 crawl_class.py[line:304] WARNING 版缂哄け>>>
2018-10-10 09:18:05 crawl_class.py[line:304] WARNING 版缂哄け>>>
2018-10-10 09:18:07 crawl_class.py[line:304] WARNING 版缂哄け>>>
2018-10-10 09:18:12 crawl_class.py[line:304] WARNING 版缂哄け>>>
2018-10-10 09:18:12 crawl_class.py[line:304] WARNING 版缂哄け>>>
2018-10-10 09:18:20 crawl_class.py[line:164] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=100>>>>>>
2018-10-10 09:21:19 crawl_class.py[line:305] WARNING 版缂哄け>>>
2018-10-10 09:22:10 crawl_class.py[line:305] WARNING 版缂哄け>>>
2018-10-10 09:23:52 crawl_class.py[line:164] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=100>>>>>>
2018-10-10 09:25:32 crawl_class.py[line:307] WARNING 版缂哄け>>>
2018-10-10 09:25:35 crawl_class.py[line:307] WARNING 版缂哄け>>>
2018-10-10 09:27:51 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=100>>>>>>
2018-10-10 09:31:55 crawl_class.py[line:308] WARNING 版缂哄け>>>
2018-10-10 09:34:31 crawl_class.py[line:308] WARNING 版缂哄け>>>
2018-10-10 09:34:33 crawl_class.py[line:308] WARNING 版缂哄け>>>
2018-10-10 09:34:52 crawl_class.py[line:308] WARNING 版缂哄け>>>
2018-10-10 09:34:53 crawl_class.py[line:308] WARNING 版缂哄け>>>
2018-10-10 09:36:12 crawl_class.py[line:308] WARNING 版缂哄け>>>
2018-10-10 09:47:16 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=100>>>>>>
2018-10-10 09:53:13 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=100>>>>>>
2018-10-10 09:55:59 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=59>>>>>>
2018-10-10 09:56:01 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=60>>>>>>
2018-10-10 09:56:04 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=61>>>>>>
2018-10-10 09:56:07 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=62>>>>>>
2018-10-10 09:56:10 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=63>>>>>>
2018-10-10 09:56:13 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=64>>>>>>
2018-10-10 09:56:16 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=65>>>>>>
2018-10-10 09:56:19 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=66>>>>>>
2018-10-10 09:56:21 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=67>>>>>>
2018-10-10 09:56:24 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=68>>>>>>
2018-10-10 09:56:27 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=69>>>>>>
2018-10-10 09:56:30 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=70>>>>>>
2018-10-10 09:56:32 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=71>>>>>>
2018-10-10 09:56:35 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=72>>>>>>
2018-10-10 09:56:37 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=73>>>>>>
2018-10-10 09:56:40 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=74>>>>>>
2018-10-10 09:56:42 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=75>>>>>>
2018-10-10 09:56:45 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=76>>>>>>
2018-10-10 09:56:48 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=77>>>>>>
2018-10-10 09:56:50 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=78>>>>>>
2018-10-10 09:56:53 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=79>>>>>>
2018-10-10 09:56:55 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=80>>>>>>
2018-10-10 09:56:58 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=81>>>>>>
2018-10-10 09:57:00 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=82>>>>>>
2018-10-10 09:57:03 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=83>>>>>>
2018-10-10 09:57:05 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=84>>>>>>
2018-10-10 09:57:08 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=85>>>>>>
2018-10-10 09:57:11 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=86>>>>>>
2018-10-10 09:57:13 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=87>>>>>>
2018-10-10 09:57:16 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=88>>>>>>
2018-10-10 09:57:18 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=89>>>>>>
2018-10-10 09:57:21 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=90>>>>>>
2018-10-10 09:57:23 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=91>>>>>>
2018-10-10 09:57:26 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=92>>>>>>
2018-10-10 09:57:28 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=93>>>>>>
2018-10-10 09:57:31 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=94>>>>>>
2018-10-10 09:57:33 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=95>>>>>>
2018-10-10 09:57:36 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=96>>>>>>
2018-10-10 09:57:38 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=97>>>>>>
2018-10-10 09:57:41 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=98>>>>>>
2018-10-10 09:57:43 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=99>>>>>>
2018-10-10 09:57:46 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=100>>>>>>
2018-10-10 10:01:29 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=100>>>>>>
2018-10-10 10:03:30 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=1>>>>>>
2018-10-10 10:03:32 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=2>>>>>>
2018-10-10 10:03:35 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=3>>>>>>
2018-10-10 10:03:37 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=4>>>>>>
2018-10-10 10:03:39 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=5>>>>>>
2018-10-10 10:03:42 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=6>>>>>>
2018-10-10 10:03:44 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=7>>>>>>
2018-10-10 10:03:46 crawl_class.py[line:165] WARNING 璇娆℃拌揪颁 宸茬姝http://xyq.cbg.163.com/cgi-bin/xyq_overall_search.py?act=overall_search_role&page=8>>>>>>
